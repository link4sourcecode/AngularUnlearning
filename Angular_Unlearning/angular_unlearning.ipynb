{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# angular unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from  torchvision import datasets\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import random \n",
    "import copy\n",
    "import time\n",
    "\n",
    "from models import init_params as w_init\n",
    "from models import TargetNet, AllCNN\n",
    "from trainer import train, eval, loss_picker, optimizer_picker\n",
    "\n",
    "from torch.backends import cudnn\n",
    "cudnn.benchmark = False      # if benchmark=True, deterministic will be False\n",
    "cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "data_name = 'cifar10'\n",
    "batch_size = 64#\n",
    "\n",
    "num_epochs = 30\n",
    "learning_rate = 0.01\n",
    "loss_mode = 'cross'\n",
    "optimization = 'sgd'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)           \n",
    "torch.cuda.manual_seed(seed)      \n",
    "torch.cuda.manual_seed_all(seed)   \n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001AE6E43BB50>\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(data_name):\n",
    "    \n",
    "    #model: 2 conv. layers followed by 2 FC layers\n",
    "    if(data_name == 'mnist'):\n",
    "        trainset = datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "        testset = datasets.MNIST('./data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "        \n",
    "    #model: ResNet-50\n",
    "    if(data_name == 'cifar10'):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                                download=True, transform=transform)\n",
    "        testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                                download=True, transform=transform)\n",
    "    \n",
    "    if(data_name == 'cifar100'):\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5074, 0.4867, 0.4411), (0.2675, 0.2565, 0.2761)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5074, 0.4867, 0.4411), (0.2675, 0.2565, 0.2761)),\n",
    "        ])\n",
    "        trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "        testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    return trainset, testset\n",
    "\n",
    "def get_dataloader(trainset, testset, batch_size, device):\n",
    "    train_loader  = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "trainset, testset = get_dataset(data_name)\n",
    "train_loader, test_loader = get_dataloader(trainset, testset, batch_size, device=device)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and save an original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from models import *\n",
    " \n",
    "num_classes = max(train_loader.dataset.targets) + 1 \n",
    "if data_name == 'cifar10':\n",
    "    model = AllCNN(n_channels=3, num_classes=num_classes, filters_percentage=0.5).to(device)\n",
    "    num_epochs = 30\n",
    "if data_name == 'cifar100':\n",
    "    normalization = NormalizeByChannelMeanStd(\n",
    "            mean=[0.5071, 0.4866, 0.4409], std=[0.2673, 0.2564, 0.2762]\n",
    "        )\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 100)\n",
    "    model.normalize = normalization\n",
    "    model = model.to(device)\n",
    "    num_epochs = 182\n",
    "    learning_rate = 0.1\n",
    "elif data_name == 'tiny-imagenet':\n",
    "    normalization = NormalizeByChannelMeanStd(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    model = resnet34(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 200)\n",
    "    model.normalize = normalization\n",
    "    model = model.to(device)\n",
    "    num_epochs = 200\n",
    "    learning_rate = 0.1\n",
    "\n",
    "\n",
    "criterion = loss_picker('cross')\n",
    "optimizer = optimizer_picker(optimization, model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "start = time.perf_counter()\n",
    "for epo in range(num_epochs):\n",
    "    train(model=model, data_loader=train_loader, criterion=criterion, optimizer=optimizer, loss_mode='cross', device=device)\n",
    "    _, acc = eval(model=model, data_loader=test_loader, mode='', print_perform=False, device=device)\n",
    "    print('EPOCH[%d]: test acc:%.4f'%(epo+1, acc))\n",
    "    scheduler.step()\n",
    "\n",
    "    if acc >= best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model, './checkpoints/original_best_%s.pth'%data_name)\n",
    "end = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrain an model for remain data \n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_dataloader(trainset, testset, batch_size, device):\n",
    "    train_loader  = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def split_class_data(dataset, forget_class, num_forget):\n",
    "    forget_index = []\n",
    "    class_remain_index = []\n",
    "    remain_index = []\n",
    "    sum = 0\n",
    "    for i, (data, target) in enumerate(dataset):\n",
    "        if target == forget_class and sum < num_forget:\n",
    "            forget_index.append(i)\n",
    "            sum += 1\n",
    "        elif target == forget_class and sum >= num_forget:\n",
    "            class_remain_index.append(i)\n",
    "            remain_index.append(i)\n",
    "            sum += 1\n",
    "        else:\n",
    "            remain_index.append(i)\n",
    "    return forget_index, remain_index, class_remain_index\n",
    "\n",
    "def get_class_unlearn_loader(trainset, testset, forget_class, batch_size, num_forget, repair_num_ratio=0.01):\n",
    "    train_forget_index, train_remain_index, class_remain_index = split_class_data(trainset, forget_class, num_forget=num_forget)\n",
    "    test_forget_index, test_remain_index, _ = split_class_data(testset, forget_class, num_forget=len(testset))\n",
    "\n",
    "    repair_class_index = random.sample(class_remain_index, int(repair_num_ratio*len(class_remain_index)))\n",
    "\n",
    "    train_forget_sampler = SubsetRandomSampler(train_forget_index)#5000\n",
    "    train_forget_sampler_0_5 =  SubsetRandomSampler(train_forget_index[::2])\n",
    "    train_remain_index_0_5 = list((set(train_forget_index) - set(train_forget_index[::2])).union(set(train_remain_index)))\n",
    "    train_remain_sampler_0_5 =  SubsetRandomSampler(train_remain_index_0_5)\n",
    "\n",
    "    train_forget_sampler_0_2 =  SubsetRandomSampler(train_forget_index[::5])\n",
    "    train_remain_index_0_2 = list((set(train_forget_index) - set(train_forget_index[::5])).union(set(train_remain_index)))\n",
    "    train_remain_sampler_0_2 =  SubsetRandomSampler(train_remain_index_0_2)\n",
    "\n",
    "    train_forget_sampler_0_1 =  SubsetRandomSampler(train_forget_index[::10])\n",
    "    train_remain_index_0_1 = list((set(train_forget_index) - set(train_forget_index[::10])).union(set(train_remain_index)))\n",
    "    train_remain_sampler_0_1 =  SubsetRandomSampler(train_remain_index_0_1)\n",
    "\n",
    "    train_forget_sampler_0_05 =  SubsetRandomSampler(train_forget_index[::20])\n",
    "    train_remain_index_0_05 = list((set(train_forget_index) - set(train_forget_index[::20])).union(set(train_remain_index)))\n",
    "    train_remain_sampler_0_05 =  SubsetRandomSampler(train_remain_index_0_05)\n",
    "\n",
    "    train_forget_sampler_0_01 =  SubsetRandomSampler(train_forget_index[::100])\n",
    "    train_remain_index_0_01 = list((set(train_forget_index) - set(train_forget_index[::100])).union(set(train_remain_index)))\n",
    "    train_remain_sampler_0_01 =  SubsetRandomSampler(train_remain_index_0_01)\n",
    "\n",
    "\n",
    "    train_remain_sampler = SubsetRandomSampler(train_remain_index)#45000\n",
    "\n",
    "    repair_class_sampler = SubsetRandomSampler(repair_class_index)\n",
    "\n",
    "    test_forget_sampler = SubsetRandomSampler(test_forget_index)#1000\n",
    "    test_remain_sampler = SubsetRandomSampler(test_remain_index)#9000\n",
    "\n",
    "    train_forget_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_forget_sampler)\n",
    "    train_forget_loader_0_5 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_forget_sampler_0_5)\n",
    "    train_forget_loader_0_2 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_forget_sampler_0_2)\n",
    "    train_forget_loader_0_1 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_forget_sampler_0_1)\n",
    "    train_forget_loader_0_05 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_forget_sampler_0_05)\n",
    "    train_forget_loader_0_01 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_forget_sampler_0_01)\n",
    "\n",
    "    train_remain_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_remain_sampler)\n",
    "    train_remain_loader_0_5 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_remain_sampler_0_5)\n",
    "    train_remain_loader_0_2 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_remain_sampler_0_2)\n",
    "    train_remain_loader_0_1 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_remain_sampler_0_1)\n",
    "    train_remain_loader_0_05 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_remain_sampler_0_05)\n",
    "    train_remain_loader_0_01 = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_remain_sampler_0_01)\n",
    "\n",
    "    repair_class_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=repair_class_sampler)\n",
    "\n",
    "    test_forget_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, sampler=test_forget_sampler)\n",
    "    test_remain_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, sampler=test_remain_sampler)\n",
    "\n",
    "    return train_forget_loader, train_forget_loader_0_5, train_forget_loader_0_2, train_forget_loader_0_1, train_forget_loader_0_05, train_forget_loader_0_01, \\\n",
    "    train_remain_loader, train_remain_loader_0_5, train_remain_loader_0_2, train_remain_loader_0_1, train_remain_loader_0_05, train_remain_loader_0_01, \\\n",
    "        test_forget_loader, test_remain_loader, repair_class_loader, train_forget_index, train_remain_index, test_forget_index, test_remain_index\n",
    "    \n",
    "\n",
    "def get_unlearn_loader(trainset, testset, forget_ratio=0.05, batch_size=64):\n",
    "    \n",
    "    index = np.arange(len(trainset))\n",
    "    test_index = np.arange(len(testset))\n",
    "    num_to_forget = int(len(index)*forget_ratio)\n",
    "    forget_index = np.random.RandomState(seed).choice(index, num_to_forget, replace=False).tolist()\n",
    "    remain_index = list(set(index) - set(forget_index))\n",
    "    remain_index = sorted(remain_index, key=lambda x: random.random())\n",
    "    val_index = np.random.RandomState(seed).choice(test_index, num_to_forget, replace=False).tolist()\n",
    "    train_forget_sampler = SubsetRandomSampler(forget_index)#5000\n",
    "    train_remain_sampler = SubsetRandomSampler(remain_index)#40000\n",
    "    test_val_sampler = SubsetRandomSampler(val_index)#5000\n",
    "\n",
    "    sorted_train_sampler = SequentialSampler(remain_index + forget_index)\n",
    "\n",
    "    train_forget_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_forget_sampler, shuffle=False, num_workers=0)\n",
    "    train_remain_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=train_remain_sampler, shuffle=False, num_workers=0)\n",
    "    test_val_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, sampler=test_val_sampler, num_workers=0)\n",
    "    sorted_train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, sampler=sorted_train_sampler, shuffle=False, num_workers=0)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    return train_forget_loader, train_remain_loader, test_val_loader, test_loader,sorted_train_loader, forget_index, remain_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            data, target = batch\n",
    "            output = model(data)\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "    return torch.cat(prob)\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):    \n",
    "    retain_prob = collect_prob(retain_loader, model)[0:40000:4,:]\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    " \n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "    Y_r = np.concatenate([np.ones(retain_prob.size(0)), np.zeros(test_prob.size(0))])\n",
    "    \n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])    \n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "    \n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model) \n",
    "    clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(X_r, Y_r)\n",
    "    results = clf.predict(X_f)\n",
    "    return results.mean()\n",
    "\n",
    "def get_entropy(retain_loader,forget_loader,test_loader, model):\n",
    "    retain_prob = collect_prob(retain_loader, model) \n",
    "    forget_prob = collect_prob(forget_loader, model) \n",
    "    test_prob = collect_prob(test_loader, model) \n",
    "    retain_entropy = entropy(retain_prob)\n",
    "    forget_entropy = entropy(forget_prob)\n",
    "    test_entropy = entropy(test_prob)\n",
    "    return retain_entropy, forget_entropy, test_entropy\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model,model_name, printable=True):\n",
    "    prob = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "    if printable == True:\n",
    "        print(\"Attack prob for %s: %f\" % (model_name, prob))\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and save a retrain model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Retrained model] ASR:{0.6754}, all test acc:{0.8141}, forget acc:{0.8224}, remain acc:{0.9840}\n",
      "[Retrained model] ASR:{0.6828}, all test acc:{0.8332}, forget acc:{0.8394}, remain acc:{0.9960}\n",
      "[Retrained model] ASR:{0.6752}, all test acc:{0.8148}, forget acc:{0.8300}, remain acc:{0.9900}\n",
      "[Retrained model] ASR:{0.6804}, all test acc:{0.7977}, forget acc:{0.8120}, remain acc:{0.9675}\n",
      "[Retrained model] ASR:{0.6820}, all test acc:{0.8231}, forget acc:{0.8374}, remain acc:{0.9938}\n",
      "Forget Ratio:[0.1000]\n",
      "[854.7909898999997, 853.8453309999995, 853.6774986, 803.2181195000012, 803.8463704999995]\n",
      "TA:{0.81658}({0.01170}), UA:{0.82824}({0.01009}), RA:{0.98624}({0.01020}), ASR:{0.67916}({0.00325}), time:{833.87566}({24.77900})\n"
     ]
    }
   ],
   "source": [
    "from models import init_params as w_init\n",
    "from models import TargetNet, AllCNN\n",
    "from trainer import train, eval, loss_picker, optimizer_picker\n",
    "\n",
    "ori_model = torch.load('./checkpoints/original_best_cifar10.pth', map_location=torch.device('cpu')).to(device)\n",
    "\n",
    "# forget_class = 4\n",
    "num_forget = 5000\n",
    "num_epochs = 27\n",
    "\n",
    "for forget_ratio in [0.1]:\n",
    "    train_forget_loader, train_remain_loader, train_val_loader, test_loader, _, forget_index, remain_index \\\n",
    "            = get_unlearn_loader(trainset, testset, forget_ratio=forget_ratio, batch_size=64)\n",
    "    num_classes = max(train_loader.dataset.targets) + 1 #if args.num_classes is None else args.num_classes\n",
    "    \n",
    "    UA, RA, TA, MIA, TC = [], [], [], [], []\n",
    "\n",
    "    for cnt in range(5):\n",
    "        \n",
    "        retrain_model = AllCNN(n_channels=3, num_classes=num_classes, filters_percentage=0.5).to(device)\n",
    "        criterion = loss_picker('cross')\n",
    "        optimizer = optimizer_picker(optimization, retrain_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0)\n",
    "        best_acc = 0\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        for epo in range(num_epochs):\n",
    "            train(model=retrain_model, data_loader=train_remain_loader, criterion=criterion, optimizer=optimizer, loss_mode='cross', device=device)\n",
    "            _, acc = eval(model=retrain_model, data_loader=test_loader, mode='', print_perform=False, device=device)#class-wise\n",
    "            print('EPOCH[{}]; test acc:{}'.format(epo+1, acc))\n",
    "\n",
    "            if acc >= best_acc:\n",
    "                best_acc = acc\n",
    "                torch.save(retrain_model, './checkpoints/retrain_best_cifar10_sample_level_forget_ratio{%.4f}.pth'%forget_ratio)\n",
    "        end = time.perf_counter()\n",
    "        tm = end-start\n",
    "\n",
    "        _, all_test_acc = eval(model=retrain_model, data_loader=test_loader, mode='', print_perform=False, device=device)\n",
    "        _, remain_train_acc = eval(model=retrain_model, data_loader=train_remain_loader, mode='', print_perform=False, device=device)\n",
    "        _, forget_train_acc = eval(model=retrain_model, data_loader=train_forget_loader, mode='', print_perform=False, device=device)\n",
    "        prob = membership_attack(train_remain_loader, train_forget_loader, test_loader, retrain_model, model_name='unlearn_model', printable=False)\n",
    "        print(\"[Retrained model] ASR:{%.4f}, all test acc:{%.4f}, forget acc:{%.4f}, remain acc:{%.4f}\"%(prob, all_test_acc, forget_train_acc, remain_train_acc))\n",
    "        \n",
    "        TA.append(all_test_acc.cpu().detach().numpy())\n",
    "        UA.append(forget_train_acc.cpu().detach().numpy())\n",
    "        RA.append(remain_train_acc.cpu().detach().numpy())\n",
    "        MIA.append(prob)\n",
    "        TC.append(tm)\n",
    "        \n",
    "    print(\"Forget Ratio:[%.4f]\"%forget_ratio)\n",
    "    print(TC)\n",
    "    print('TA:{%.5f}({%.5f}), UA:{%.5f}({%.5f}), RA:{%.5f}({%.5f}), ASR:{%.5f}({%.5f}), time:{%.5f}({%.5f})'\\\n",
    "            %(np.mean(TA), np.std(TA), np.mean(UA), np.std(UA), \\\n",
    "                np.mean(RA), np.std(RA), np.mean(MIA), np.std(MIA),\\\n",
    "                    np.mean(TC), np.std(TC)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## angular unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "[m1 = 0.40] [m2 = 1.00]\n",
      "Forget Ratio:[0.1000]\n",
      "[Unlearned model] ASR-U:{0.6512}, ASR-R:{0.9444}, all test acc:{0.8341}, forget acc:{0.8564}, remain acc:{0.9955}\n",
      "Forget Ratio:[0.1000]\n",
      "[Unlearned model] ASR-U:{0.6468}, ASR-R:{0.9430}, all test acc:{0.8248}, forget acc:{0.8426}, remain acc:{0.9956}\n",
      "Forget Ratio:[0.1000]\n",
      "[Unlearned model] ASR-U:{0.6482}, ASR-R:{0.9344}, all test acc:{0.8274}, forget acc:{0.8470}, remain acc:{0.9946}\n",
      "Forget Ratio:[0.1000]\n",
      "[Unlearned model] ASR-U:{0.6648}, ASR-R:{0.9367}, all test acc:{0.8316}, forget acc:{0.8438}, remain acc:{0.9929}\n",
      "Forget Ratio:[0.1000]\n",
      "[Unlearned model] ASR-U:{0.6634}, ASR-R:{0.9340}, all test acc:{0.8224}, forget acc:{0.8442}, remain acc:{0.9934}\n",
      "Forget Ratio:[0.1000]\n",
      "[90.91720250000071, 94.9629666999972, 93.5495166000037, 94.29275659999985, 94.46772010000132]\n",
      "TA:{0.82806}({0.00429}), UA:{0.84680}({0.00501}), RA:{0.99441}({0.00108}), ASR:{0.65488}({0.00767}), time:{93.63803}({1.43410})\n"
     ]
    }
   ],
   "source": [
    "from utils import inf_generator\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "def collect_feature_mean_std(model, loader, num_class):\n",
    "    \n",
    "    feats, labels = [], []\n",
    "    model = model.to(device)\n",
    "    for data,label in loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        feat = model.features(data)#\n",
    "        feats.append(feat.detach())\n",
    "        labels.append(label)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0) \n",
    "    labels = torch.cat(labels, dim=0) \n",
    "\n",
    "    assert feats.shape[0] == labels.shape[0]\n",
    "    class_mean = torch.zeros(num_class, feats.shape[-1])\n",
    "    class_std = torch.zeros(num_class, feats.shape[-1])\n",
    "    class_cov = [torch.zeros((feats.shape[1], feats.shape[1]))]*num_class\n",
    "\n",
    "    for c in range(num_class):\n",
    "        index = torch.where(labels == c)[0]\n",
    "        if index.size(0) == 0:\n",
    "            _std, _mean = torch.zeros(num_class), torch.zeros(num_class)\n",
    "        else:\n",
    "            _std, _mean = torch.std_mean(feats[index], dim=0, unbiased=True)\n",
    "        class_mean[c] = _mean#[10]\n",
    "        class_std[c] = _std#[10] \n",
    "        class_cov[c] = torch.cov(feats[index].T).cpu().detach().numpy()\n",
    "    \n",
    "    return class_mean.cpu().detach().numpy(), class_std.cpu().detach().numpy(), class_cov, feats, labels\n",
    "\n",
    "def collect_angular_info(model, loader, num_class):\n",
    "    \n",
    "    feats, labels = [], []\n",
    "    model = model.to(device)\n",
    "    for data,label in loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        feat = model(data)\n",
    "        feats.append(feat.detach())\n",
    "        labels.append(label)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0) \n",
    "    labels = torch.cat(labels, dim=0) \n",
    "\n",
    "    assert feats.shape[0] == labels.shape[0]\n",
    "    class_mean = torch.zeros(num_class, feats.shape[-1])\n",
    "    class_std = torch.zeros(num_class, feats.shape[-1])\n",
    "    class_cov = [torch.zeros((feats.shape[1], feats.shape[1]))]*num_class\n",
    "\n",
    "    for c in range(num_class):\n",
    "        index = torch.where(labels == c)[0]\n",
    "        if index.size(0) == 0:\n",
    "            _std, _mean = torch.zeros(num_class), torch.zeros(num_class)\n",
    "        else:\n",
    "            _std, _mean = torch.std_mean(feats[index], dim=0, unbiased=True)\n",
    "        class_mean[c] = _mean#[10]\n",
    "        class_std[c] = _std#[10]\n",
    "        class_cov[c] = torch.cov(feats[index].T).cpu().detach().numpy()\n",
    "\n",
    "    class_mean = class_mean.to(device)\n",
    "    class_center = [p/p.norm() for p in class_mean]\n",
    "\n",
    "    distances = torch.cdist(feats, torch.stack(class_center))\n",
    "    min_dist, indices = torch.min(distances, dim=1)#_, \n",
    "    sample_class_centroids = torch.stack(class_center)[indices]\n",
    "\n",
    "    distances.scatter_(1, indices.unsqueeze(1), float('inf'))\n",
    "    min_other_dist, nearest_other_labels = torch.min(distances, dim=1)\n",
    "    nearest_other_centroids = torch.stack(class_center)[nearest_other_labels]\n",
    "    \n",
    "    element_angular = []\n",
    "    element_angular_no = []\n",
    "    for i in range(labels.size(0)):\n",
    "        element_angular.append(np.degrees(torch.acos(torch.dot(feats[i]/feats[i].norm(), class_center[labels[i]])).cpu().detach().numpy()))#.cpu().detach().numpy())\n",
    "        element_angular_no.append(np.degrees(torch.acos(torch.dot(feats[i]/feats[i].norm(), nearest_other_centroids[i])).cpu().detach().numpy()))\n",
    "\n",
    "    return element_angular, element_angular_no, class_center, sample_class_centroids, nearest_other_centroids\n",
    "\n",
    "def get_element_cos(features, centers): \n",
    "    normed_feats = features / torch.norm(features, dim=1, keepdim=True)\n",
    "    normed_centers = centers / torch.norm(centers, dim=1, keepdim=True)\n",
    "    cos_sim = torch.matmul(normed_feats, normed_centers.T)\n",
    "\n",
    "    return cos_sim\n",
    "\n",
    "def get_batch_angulars(features, labels, centroids, model):\n",
    "    params = model.parameters()#.clone().detach()\n",
    "    w = list(params)[-2:][0].clone().detach()\n",
    "    features = features / torch.norm(features, dim=1, keepdim=True)\n",
    "    center = centroids / torch.norm(centroids, dim=1, keepdim=True)\n",
    "\n",
    "    w = w / w.norm()\n",
    "    distances = torch.mm(features, center.t().contiguous())\n",
    "    \n",
    "    min_dist, indices = torch.max(distances, dim=1)\n",
    "    batch_centroids =centroids[labels]\n",
    "    batch_avh = avh(distances, labels)\n",
    "\n",
    "    distances.scatter_(1, indices.unsqueeze(1), float('-inf'))\n",
    "    min_other_dist, nearest_other_labels = torch.max(distances, dim=1)\n",
    "    batch_nearest_centroids = centroids[nearest_other_labels]\n",
    "\n",
    "    return batch_centroids, batch_nearest_centroids, batch_avh.detach(), nearest_other_labels\n",
    "\n",
    "\n",
    "def avh(cosine_dists, targets):\n",
    "    \"\"\"'\n",
    "    @param cosine_dists: B x C\n",
    "    @param targets: C\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    ang_dists = torch.acos(torch.clamp(cosine_dists, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "    avh = (\n",
    "          ( ang_dists.gather(1, targets[:, None]).squeeze()) / (  ang_dists.sum(1, keepdim=True).squeeze())\n",
    "    )\n",
    "    return avh #\n",
    "\n",
    "def AngLoss(feature, label, weight, s=None, m=0.35, avh=None):\n",
    "    feat_norm = F.normalize(feature, p=2, dim=1)\n",
    "    weight_norm =F.normalize(weight, p=2, dim=1)\n",
    "    cosine = torch.mm(feat_norm, weight_norm.t().contiguous())\n",
    "    \n",
    "    m_hot = nn.functional.one_hot(label, num_classes=cosine.size(1)) * m\n",
    "    output = cosine - m_hot\n",
    "    if s != None:\n",
    "        output *= s\n",
    "    else:\n",
    "        output *= torch.norm(feature, p=2, dim=1).view(-1, 1) \n",
    "    loss = F.cross_entropy(output, label)\n",
    "\n",
    "    return loss\n",
    "\n",
    "unlearn_epoch= 10\n",
    "num_class=10\n",
    "\n",
    "\n",
    "for m1 in [  0.4]:\n",
    "    for m2 in [  1.0]: \n",
    "        print(\"=\"*100)\n",
    "        print('[m1 = %.2f] [m2 = %.2f]'%(m1, m2))\n",
    "        UA, RA, TA, MIA, TC = [], [], [], [], []\n",
    "        for forget_ratio in [ 0.1,0.1,0.1,0.1,0.1]:\n",
    "            train_forget_loader, train_remain_loader, train_val_loader, test_loader, sorted_train_loader, forget_index, remain_index \\\n",
    "                = get_unlearn_loader(trainset, testset, forget_ratio=forget_ratio, batch_size=64)\n",
    "            retrain_model = torch.load('./checkpoints/retrain_best_cifar10_sample_level_forget_ratio{%.4f}.pth'%forget_ratio, map_location=torch.device('cpu')).to(torch.device('cuda'))    \n",
    "            ori_model = torch.load('./checkpoints/original_best_cifar10.pth', map_location=torch.device('cpu')).to(torch.device('cuda'))\n",
    "            test_model = copy.deepcopy(ori_model).to(device)\n",
    "            unlearn_model = copy.deepcopy(ori_model).to(device)\n",
    "            class_center, _, _, _, _ = collect_feature_mean_std(test_model, train_forget_loader, num_class=10)\n",
    "            class_center = torch.tensor(class_center).to(device)\n",
    "            forget_data_gen = inf_generator(train_forget_loader)\n",
    "            repair_data_gen = inf_generator(train_remain_loader)\n",
    "            batches_per_epoch = len(train_forget_loader)\n",
    "\n",
    "            criterion = loss_picker('cross')\n",
    "            optimizer = optimizer_picker(optimization, unlearn_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            list_sim_cno = []\n",
    "            list_sim_c = []\n",
    "            norm_of_Df = []\n",
    "            num_of_hs = []\n",
    "            margin_avh = []\n",
    "            \n",
    "            for itr in range(unlearn_epoch * batches_per_epoch):\n",
    "                \n",
    "                x, y = forget_data_gen.__next__()\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                xr, yr = repair_data_gen.__next__()\n",
    "                xr, yr = xr.to(device), yr.to(device)\n",
    "\n",
    "                centers = class_center[y]\n",
    "                centers = centers.to(device)\n",
    "                #target label\n",
    "\n",
    "                unlearn_model.train()\n",
    "                unlearn_model.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                features = unlearn_model.features(x)\n",
    "                ref_features = unlearn_model.features(xr)\n",
    "                batch_centroids, batch_nearest_centroids, batch_avh, nearest_other_labels = get_batch_angulars(features, y, class_center, unlearn_model)\n",
    "                _, _, _, ori_nearest_other_labels = get_batch_angulars(features, y, class_center, test_model)\n",
    "                params = unlearn_model.parameters()\n",
    "                weight = list(params)[-2:][0]\n",
    "                weight_norm =F.normalize(weight, p=2, dim=1)\n",
    "\n",
    "                easy_indice = torch.where(batch_avh < batch_avh.mean())[0]\n",
    "                hard_indice = torch.where(batch_avh > batch_avh.mean())[0]\n",
    "                loss_norm =  0.5*torch.norm(features, p=2, dim=1).mean()\n",
    "                \n",
    "                loss_Df = AngLoss(features[hard_indice], ori_nearest_other_labels[hard_indice], weight,s=40,m=0)#\n",
    "                loss_Dr = AngLoss(ref_features, yr, weight, s=40, m=0.45) \n",
    "                loss =    0.3*loss_Df+ 0.8*loss_Dr\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            tm = end-start\n",
    "            # print('Time Consuming:', tm, 'secs' )\n",
    "\n",
    "            # torch.save(unlearn_model, './checkpoints/unlearned_model_cifar10_ratio%.3f.pth'%(forget_ratio))\n",
    "\n",
    "            print(\"Forget Ratio:[%.4f]\"%forget_ratio)\n",
    "            _, all_test_acc = eval(model=unlearn_model, data_loader=test_loader, mode='', print_perform=False, device=device)\n",
    "            _, remain_train_acc = eval(model=unlearn_model, data_loader=train_remain_loader, mode='', print_perform=False, device=device)\n",
    "            _, forget_train_acc = eval(model=unlearn_model, data_loader=train_forget_loader, mode='', print_perform=False, device=device)\n",
    "            prob = membership_attack(train_remain_loader, train_forget_loader, test_loader, unlearn_model, model_name='unlearn_model', printable=False)\n",
    "            remain_prob = membership_attack(train_remain_loader, train_remain_loader, test_loader, unlearn_model, model_name='unlearn_model', printable=False)\n",
    "            print(\"[Unlearned model] ASR-U:{%.4f}, ASR-R:{%.4f}, all test acc:{%.4f}, forget acc:{%.4f}, remain acc:{%.4f}\"%(prob, remain_prob,all_test_acc, forget_train_acc, remain_train_acc))\n",
    "\n",
    "            TA.append(all_test_acc.cpu().detach().numpy())\n",
    "            UA.append(forget_train_acc.cpu().detach().numpy())\n",
    "            RA.append(remain_train_acc.cpu().detach().numpy())\n",
    "            MIA.append(prob)\n",
    "            TC.append(tm)\n",
    "            \n",
    "        print(\"Forget Ratio:[%.4f]\"%forget_ratio)\n",
    "        print(TC)\n",
    "        print('TA:{%.5f}({%.5f}), UA:{%.5f}({%.5f}), RA:{%.5f}({%.5f}), ASR:{%.5f}({%.5f}), time:{%.5f}({%.5f})'\\\n",
    "            %(np.mean(TA), np.std(TA), np.mean(UA), np.std(UA), \\\n",
    "                np.mean(RA), np.std(RA), np.mean(MIA), np.std(MIA),\\\n",
    "                    np.mean(TC), np.std(TC)))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aae1bf6e6493eccb6a1c32416dad7fcf9d70fcad32a6ac6347a4881333afa3e2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
